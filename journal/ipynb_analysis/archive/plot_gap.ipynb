{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e81b8-7cf2-4b9f-98d7-11f7587b637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded27053-b2af-4602-90d5-c89ff0445a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_predict_mem = 25146949120\n",
    "cifar_measure_mem = 23366852608\n",
    "cifar_measure_bs = 13466\n",
    "\n",
    "imagenet_predict_mem = 25120424448\n",
    "imagenet_measure_mem = 24362376704\n",
    "imagenet_measure_bs = 859\n",
    "\n",
    "mem_max = 25147867136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63899cd1-a92a-44df-88db-f056ee3a505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative error\n",
    "cifar_error = (cifar_predict_mem - cifar_measure_mem) / cifar_measure_mem\n",
    "imagenet_error = (imagenet_predict_mem - imagenet_measure_mem) / imagenet_measure_mem\n",
    "print(f'cifar relative error: {cifar_error: .3f}')\n",
    "print(f'imagenet relative error: {imagenet_error: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772b668-8df9-4dc6-85fc-cf9a71b97885",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = ['CIFAR-100', 'ImageNet']\n",
    "mem = {\n",
    "    'prediction': [i / 2**20 for i in [cifar_predict_mem, imagenet_predict_mem]],\n",
    "    'measurement': [i / 2**20 for i in [cifar_measure_mem, imagenet_measure_mem]],\n",
    "}\n",
    "x = np.arange(len(species))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, value in mem.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, value, width, label=attribute)\n",
    "    ax.bar_label(rects)\n",
    "    multiplier += 1\n",
    "\n",
    "GPU_MiB = mem_max / 2**20\n",
    "ax.axhline(y=GPU_MiB, color='red', linestyle='--')\n",
    "trans = transforms.blended_transform_factory(ax.get_yticklabels()[0].get_transform(), ax.transData)\n",
    "ax.text(0, GPU_MiB, \"{:.0f}\".format(GPU_MiB), color=\"red\", transform=trans, ha=\"right\", va=\"center\")\n",
    "\n",
    "ax.set_ylabel('Memory Usage (MiB)')\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_title('Memory Usage for Training ResNet-18')\n",
    "ax.set_xticks(x + width / len(species), species)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('mem_usage_gap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3879bc0-cf0b-4c3d-be88-2ab4c3fa395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for print image\n",
    "#\"\"\"\n",
    "species = ['CIFAR-100', 'ImageNet']\n",
    "mem = {\n",
    "    'prediction': [i / 2**20 for i in [cifar_predict_mem, imagenet_predict_mem]],\n",
    "    'measurement': [i / 2**20 for i in [cifar_measure_mem, imagenet_measure_mem]],\n",
    "}\n",
    "x = np.arange(len(species))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "\n",
    "for attribute, value in mem.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, value, width, label=attribute)\n",
    "    ax.bar_label(rects)\n",
    "    multiplier += 1\n",
    "\n",
    "GPU_MiB = mem_max / 2**20\n",
    "ax.axhline(y=GPU_MiB, color='red', linestyle='--')\n",
    "trans = transforms.blended_transform_factory(ax.get_yticklabels()[0].get_transform(), ax.transData)\n",
    "ax.text(0, GPU_MiB, \"{:.0f}\".format(GPU_MiB), color=\"red\", transform=trans, ha=\"right\", va=\"center\")\n",
    "\n",
    "ax.set_ylabel('Memory Usage (MiB)')\n",
    "ax.set_xlabel('Dataset')\n",
    "#ax.set_title('Memory Usage for Training ResNet-18')\n",
    "ax.set_xticks(x + width / len(species), species)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('mem_usage_gap.png', transparent=True)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba6021-8929-46cd-b35c-db6e27b43329",
   "metadata": {},
   "source": [
    "Dear Professor Nguyen,\n",
    "\n",
    "I apologize for any inconvenience this may have caused.\n",
    "As I have not received a response from you in about a week, I am unsure whether my previous email reached you.\n",
    "Therefore, I am sending it again.\n",
    "\n",
    "As we discussed in our previous meeting, we talked about how to automate the configuration of training settings.\n",
    "For my work, the maximum batch size $B_{max}$ requires manual input from the user.\n",
    "You suggested that I explore a method to automate this parameter.\n",
    "\n",
    "To address this, I propose integrating a program to calculate $B_{max}$ before training begins.\n",
    "As we know, PyTorch will crash if the GPU cannot allocate memory beyond its available free memory.\n",
    "In practice, users often determine $B_{max}$ through trial and error, which is also the approach I followed in my paper.\n",
    "After careful consideration, I believe it is feasible to automate $B_{max}$ by profiling memory usage.\n",
    "\n",
    "Assuming that memory usage scales linearly with batch size $B$, we can compute the approximate total memory usage $M(B)$ using the following equation:\n",
    "$$M(B) = P + B \\times (A \\times L)$$\n",
    "Here, $P$ is the memory required for model parameters, $A$ is the memory used for activations per sample, and $L$ is the number of layers in the model.\n",
    "Our goal is to find the maximum batch size $B_{max}$ such that $M(B_{max}) < M_{GPU}$, where $M_{GPU}$ is the total GPU memory.\n",
    "To achieve this, we can measure memory usage by running two batches with different sizes and calculating the memory usage gap.\n",
    "Using this gap, we can estimate the memory usage per sample, infer $M(B_{max})$, and determine $B_{max}$ with minimal additional computational overhead.\n",
    "\n",
    "I conducted an experiment to validate this approach, and the results of memory usage are shown in the attached figure.\n",
    "The discrepancy between predicted and measured values is likely due to PyTorch's memory management mechanisms.\n",
    "As the measured values are consistently lower than the predicted ones, the computed batch size $B_{comp}$ is smaller than the actual maximum batch size $B_{max}$.\n",
    "Although $B_{comp}$ is not optimal, it remains a sub-optimal yet practical solution.\n",
    "Based on this observation, we can reasonably conclude that $B_{comp}$ is both reliable and applicable in practice.\n",
    "\n",
    "I hope this approach meets your expectations, and I would greatly appreciate any further suggestions you may have.\n",
    "Thank you for your time and consideration.\n",
    "\n",
    "Sincerely,\n",
    "Kuan-Wei Lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0d24b-0c62-4ed2-9345-2cdace5c31ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
